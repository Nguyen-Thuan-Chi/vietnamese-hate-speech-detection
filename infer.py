import torch
from src.services.predictor import HateSpeechPredictor


def main():
    # Ch·ªçn model Epoch 3 (Ngon nh·∫•t)
    MODEL_PATH = "models/phobert_epoch_3.pth"

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"--> ƒêang kh·ªüi t·∫°o Predictor tr√™n {device.upper()}...")

    try:
        # L∆∞u √Ω: Model train v·ªõi n_classes=2 th√¨ l√∫c load c≈©ng ph·∫£i y h·ªát
        predictor = HateSpeechPredictor(MODEL_PATH, device=device)
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        return

    print("\n=== TEST MODEL TOXIC DETECTION (Binary) ===")
    print("M·ªùi b·∫°n nh·∫≠p c√¢u c·∫ßn test (G√µ 'exit' ƒë·ªÉ tho√°t):")

    while True:
        text = input("\n>> Nh·∫≠p: ")
        if text.lower() in ['exit', 'quit']:
            break
        if not text.strip(): continue

        result = predictor.predict(text)

        # In m√†u m√® t√≠ cho d·ªÖ nh√¨n
        label = result['label']
        conf = result['confidence']

        print("-" * 50)
        print(f"G·ªëc:   {result['text_input']}")
        print(f"S·∫°ch:  {result['text_clean']}")

        if label == "TOXIC":
            print(f"K·∫øt qu·∫£: üî¥ {label} (ƒê·ªô tin c·∫≠y: {conf})")
        else:
            print(f"K·∫øt qu·∫£: üü¢ {label} (ƒê·ªô tin c·∫≠y: {conf})")
        print("-" * 50)


if __name__ == "__main__":
    main()